# 上下文深度CNN用于高光谱图像分类 #
## 摘要 ##
- 提出的上下文深度CNN，通过利用相邻单个像素向量的局部空谱关系可充分利用局部上下文相互影响。
- 通过将多尺度卷积滤波器组作为提出CNN的初始部分来实现空谱信息的联合利用。通过多尺度卷积滤波器组获得初始的空间和光谱特征映射，然后形成联合空谱特征映射，将其输入到全卷积网络预测每个像素向量的标签。
## 引言 ##
- 将DCNN用于HSI分类存在的问题：HSI数据集大部分不可用，缺少足够的训练样本，导致具有大量参数的DCNN只能进行次优训练。无法获得大规模的HSI数据集，导致现有基于CNN的方法不能扩展更深更广以利用高光谱图像丰富的光谱信息。当前的CNN主要使用小规模的网络，层数（深度）和每层节点数（宽度）较少，性能低。更深和更宽意味着大数量的层数和每层的节点数。
- 通常使用主成分分析（PCA）、平衡局部判别嵌入（BLDE）之类技术减少HSI的光谱维度是输入数据使用小规模网络。
- 本论文旨在构建一个更深更宽的网络，即使HSI的数据有限，仍可以联合利用空谱信息。
- 图1（a）残差学习——解决在有限数据集上训练大型网络的问题，可以显著提高大型网络的训练效率。残差结构可以增加网络的深度和宽度。
- 提高HSI分类性能，联合利用空谱特征很有必要。现有方法没有充分利用光谱信息,这两种信息通过预处理分别获取,然后处理在一起用于特征提取和分类.
- 全卷积层(FCN)更好的利用空谱信息.
- 多尺度卷积滤波器组——提出的深度CNN在初始阶段利用多尺度卷积滤波器组,概念上类似"初始模块"同时扫描高光谱图像的局部区域以获得光谱和空间特征映射.多尺度卷积滤波器组用于利用多种局部空间结构和局部光谱相关性.然后将滤波器结合在一起形成联合空谱特征映射.
- 提出的网络是一个端对端(端到端指的是输入是原始数据，输出是最后的结果，非端到端的输入端不是直接的原始数据，而是在原始数据中提取的特征)的网络,无需额外的预处理和后处理既可进行优化和测试.提出的网络是全卷积(FCN)网络,可用于获取输入任意大的高光谱图像而不使用子采样(池化)层,是输出与输入有不同的大小,意味着网络可以处理任意大小的高光谱图像.
## 相关论文 ##
- A  获取更深的CNN用于目标检测/分类
- 第一个深度CNN是LeNet-5包含两个卷积层,两个全连接层和一个高斯连接层和几个池化层
- AlexNet 有五个卷积层和三个全连接层
- VGG-16有十六个卷积层
- GoogLeNet 是一个22层的深度网络,利用多尺度处理
- B 深度CNN用于高光谱图像分类
## 上下文深度卷积神经网络 ##
描述AlexNet，讨论提出网络的整体框架，详细描述多尺度卷积滤波器组和残差学习
- A 深度卷积神经网络
- 如图2所示，AlexNet是一种广泛使用的深度CNN模型，为所提出的网络结构提供了基础。 AlexNet由五个卷积层和三个全连接层组成。
（1）——全连接操作
（2）——卷积操作
 (3) ——局部响应归一化
 (4) ——Softmax
- B 提出的网络框架
- 图3,提出的网络框架第一部分是多尺度滤波器组随后是两个残差学习最后是三个卷积层类似AlexNet用于分类的全连接层,类似AlexNet网络,第七八层在训练过程dropout, ReLU在多尺度滤波器组，第二，第三，第五，第七，第八卷积层和两个残差学习模块之后使用.前两个卷积层的输出通过LRN进行归一化。所有数据块的长宽一样,深度不一样.所有的FCN过程没有降维.
- 图4,对于像素分类，卷积层可以在相同的权值数目下达到与全连通层相同的效果
- 表I所示，对于相同的训练数据集，所提出的网络在参数数量和训练数据大小之间的比率比基线[2]的比率大得多。 而且，所提出的网络的参数与数据比率至少比任何图像分类CNN的参数对数据比率大至少八倍。 这表明所提议网络的体系结构旨在确保其提供足够的层深度以充分利用训练数据
- C 多尺度滤波器组-最佳地利用输入图像的各种局部结构
- 卷积滤波器（1×1×B、3×3×B和5×5×B）局部卷积，其中B是光谱带的数目。3×3×B和5×5×B滤波器用于提取输入图像的局部空间相关性，而1×1×B滤波器用于处理光谱相关性.如图3所示，将第一卷积层的输出，即三个卷积特征图组合在一起，以形成一个联合空间光谱特征图，用作后续卷积层的输入
- 三个卷积滤波器得到的特征映射的大小不同，因此需要一种策略来调整特征映射的大小以使它们组合成一个联合特征映射。首先，在输入图像周围填充两个像素宽度的空间，填充0，使特征映射的大小(1×1、3×3和5×5)分别为变为（H+4，W+4），（H+2，W+2）和（H，W），H和W分别是输入滤波器图像的高度和宽度。将5×5和3×3最大池分别应用于1×1和3×3滤波器的特征映射后，所有特征映射的大小变为（H，W）
- 网络规模增加,少量训练样本面临过拟合,采用数据增强和残差学习模块
- D 残差学习
- (5) 残差学习公式,使用1×1×B滤波器来从联合空间光谱特征图中提取非线性特征,残差学习可以显著提高深层网络的训练速率.多尺度滤波器组和残差学习都能有效地增加网络的深度和宽度,同时限制计算成本.有助于少量训练样本训练深度网络.
- E 训练提出的网络
- Figure 5表明了训练过程,选一部分用于训练,其余用于评估.为了避免过拟合,通过在水平,垂直和对称轴上翻转样本,将训练样本增加四倍.对于每个训练像素,剪裁5×5的相邻像素用于训练卷积层.利用SGD训练网络.为了学习网络，最后一个argm

ax层被通常用于学习卷积层的softmax层代替。第一、第二和第九卷积层从零均值高斯分布初始化，标准偏差为0.01，其余卷积层初始化为标准偏差0.005。除最后一层外，所有卷积层的偏差初始化为1，最后一层初始化为零。
## 实验结果 ##
- A 数据集和基准
- 三个数据集,200作为训练集
- B HSI分类
- Table V显示了提出的网络与基准之间的比较
- Fig. 7 分类结果
- C 网络最优深度和宽度
- Table VI 核数量(网络宽度),Table VII 训练时间,在网络中添加更多的滤波器不仅导致性能下降,也会增加计算成本.
- Table VIII 不同残差块的数量,三个残差块容易过拟合.Table IX训练时间,三个残差块计算成本高
- D 多尺度滤波器组的有效性-联合利用空谱信息
- Table X 1×1无多尺度滤波器组,3×3,5×5,7×7 1×1:一是没有联合利用空间光谱信息；二是由于不存在空间滤波，无法使用镜像局部区域的数据增强,7×7:类边界附近的“溢出”引起的
- E 残差学习的有效性
- Table XI前者是没有残差模块,后者是第一个残差模块用两个卷积层代替.没用残差模块的网络没有收敛,因为训练数据小.
- Figure 9 残差学习的有效性评估,w/ residual learning是提出的框架,w/o residual learning是改进的框架,利用两个非线性层代替第一个残差块.
- F 性能随训练集大小改变
- Table XII 与基于多核学习(MKL)的高光谱分类方法比较,即使数据集比较小,分类精度还是比较高.
- G 误报分析
- TableXIII显示了三个数据集的混淆矩阵，从任意一个训练/测试部分计算。对于Indian Pines数据集低于95%的类只有两个corn- notill和soybean-mintill,这两类的样本数量比其他的大得多,光谱分布比其他类更广,这个两类误报率大约5%.TableXIII(b)记错不是100%,应该是89.4%.
- Table XIV边界上的像素标记为0,靠近边界且相距一个像素的标记为1,其余的标记为≥2.使用的是每个像素空间信息5×5的像素,每个类别的测试数据,超过一个像素不易发生误分类.
