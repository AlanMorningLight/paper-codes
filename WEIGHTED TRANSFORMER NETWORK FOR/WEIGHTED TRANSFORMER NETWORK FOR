# 摘要 #
- 具有某种形式的卷积或递归的序列对序列模型。Vaswani等人。（2017）提出一种新的架构，完全避免重复和冲突。相反，它只使用自我关注和前馈层。虽然所提出的体系结构在多个机器翻译任务上取得了最新的结果，但它需要大量的参数和训练过程才能收敛。我们提出了加权变换器，一种改进了注意层的变换器，它不仅在BLEU得分上优于基线网络，而且收敛速度快了15-40%。具体地说，我们用多个自我注意分支代替多个头部注意，这些分支在训练过程中由模型学习组合而成。

