# 基于注意辅助CNNs的高光谱图像分类 #
## 摘要 ##
- 通常的处理方法是先从高光谱图像中裁剪出小立方体，然后输入到CNNs中提取光谱和空间特征。众所周知，立方体中不同的光谱带和空间位置具有不同的分辨能力。如果充分挖掘这些先验信息，将有助于提高CNNs的学习能力。在这个方向上，我们提出了一个注意力辅助的CNN模型，用于高光谱图像的光谱空间分类。具体而言，分别提出了光谱注意子网络和空间注意子网络用于光谱分类和空间分类。它们都是基于传统的CNN模式，并加入了注意力模块，以帮助网络集中在更具辨别性的通道或位置上。在最后的分类阶段，采用自适应加权求和的方法将光谱分类结果和空间分类结果相结合。
## 引言 ##
- 提出了一种用于高光谱图像分类的双分支光谱空间注意网络。与现有的cnn相比，我们的模型在每个卷积层中加入了注意模块，使得cnn集中于更具区分性的信道和空间位置，同时抑制了不必要的信道和空间位置。在分类阶段，通过自适应加权求和法将两个分支结果融合在一起。
- 考虑到训练样本数目有限，我们提出了一个基于两个卷积算子的轻量级频谱注意模块。在卷积层之前，我们使用全局平均池来减少空间信息的影响。更重要的是，我们在模块中添加了一个输出层，以帮助其学习过程。
- 与光谱注意模块类似，我们还使用两个卷积层来构造空间注意模块。我们采用了1×1的卷积层，将信道数减少到1，而不使用池操作。此外，还增加了一个输出层来指导空间模块的学习过程。
## 方法 ##
### A. Framework of the Proposed Model ##
- 图1.提出方法的流程。它主要由两个分支组成：光谱注意子网和空间注意子网。与广泛使用的CNN不同，频谱和空间注意子网络结合了注意模块，细化了每个卷积层的特征映射，从而提高了CNN的学习能力。具体来说，对于给定的像素，首先提取一个以像素为中心的小立方体。然后，将立方体同时送入光谱注意子网和空间注意子网，得到两个分类结果。最后，采用加权求和法将这两个结果结合起来。
- （1）加权求和结果。其中α和β是加权参数。在整个网络的优化过程中，可以自适应地学习它们。
### B.The Structure of CNNs ###
- 采用了三个卷积层来构造光谱和空间子网络。每一个卷积层后面依次是一个批规范化层来正则化和加速训练过程，以及一个校正的线性单元（ReLU）来学习非线性表示。在第二层和第三层卷积层之前，采用最大池层来降低数据方差和计算复杂度。每个卷积层的核大小为3×3，第一层到第三层的信道数分别为32、64和128。
- （2）对于第l个卷积层，第i个特征映射。在方程（2）中，卷积算子和求和算子可以分别学习空间特征和聚集谱特征
### C. Attention Modules ###
- 对于同一物体，不同波段的光谱响应可能有很大的差异，这意味着不同波段的分辨能力是不同的。另外，不同位置的立方体也有不同的语义信息。例如，对象边缘通常比其他位置更具区分性。如果能充分利用这些先验信息，可以提高光谱和空间子网络的学习能力。为了实现这一目标，本文设计了两类注意模块。它们是光谱注意模块和空间注意模块。采用频谱注意模块，使频谱子网在抑制不必要的信道的同时，聚焦于更具分辨力的信道。同样，空间注意模块可以使空间子网更加关注语义位置。
#### Spectral Attention.
-  图2（a）所示，利用特征映射的信道间关系来构造频谱注意模块。给定中间特征映射Fl，采用全局平均池层来压缩其空间维数。然后，两个一维卷积层来生成一个光谱注意映射Aspe（Fl）
- （3）两个一维卷积层来生成一个光谱注意映射Aspe（Fl）。σ - sigmoid function，f - ReLU function ，Flavg -  obtained by the global average-pooling operator。在每个卷积层中都使用了padding，以使输出大小等于C。由于当l从1变为3时，C会增加，所以当l增大时，使用更大的k值。具体地说，当l分别等于1、2和3时，k被设置为3、5和7。
- （4）将注意力映射用于原始的特征映射，得到最具有区别的性的特征。‘⊗’元素乘法。(Fl)’是第（l+1）卷积层和输出分支（图2中的黄色）的输入。
- 输出分支由全局最大池层和完全连接层组成。这一分支主要有两个目的：一是为光谱注意力模块提供有监督的信息，以保证精细特征图的识别能力；另一个是在损失函数中加入一个正则化项，解决网络训练中过拟合问题
#### Spatial Attention
- 如图2（b）所示，首先使用1×1卷积层沿Fl的通道方向聚集信息，生成二维特征映射Ml，然后，利用两个二维卷积层得到空间注意映射Aspa（Fl）
- （5）空间注意映射Aspa（Fl），Q1和Q2具有相同的核大小，由于空间大小随着l的增加而减小，当l分别等于1、2和3时，k被设置为7、5和3。
- （6）将注意力映射用于原始的特征映射，得到最具有区别的性的特征。
- 输出分支中使用一个自适应的最大池层，这意味着对于任何大小的输入，输出大小是固定的。具体地说，当l分别等于1、2和3时，输出大小固定为4×4、2×2和1×1。
#### D. Network Training
- 为了有效地训练所提出的模型，我们采用了两个策略。第一步是对两个子网络进行独立的预训练，第二步是增加加权求和层并对整个网络进行微调。
- （7）损失值。O(j)i 第j个训练
- （8）更新损失值L。在微调过程使用梯度下降法优化L
