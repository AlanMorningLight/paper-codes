# Abstract #
- Transformer 架构早已在自然语言处理任务中得到广泛应用，但在计算机视觉领域中仍然受到限制。在计算机视觉领域，注意力要么与卷积网络结合使用，要么用来代替卷积网络的某些组件，同时保持其整体架构不变。
- 该研究表明，对 CNN 的依赖不是必需的，当直接应用于图像块序列时，transformer 也能很好地执行图像分类任务。该研究基于大量数据进行模型预训练，并迁移至多个图像识别基准数据集（ImageNet、CIFAR-100、VTAB 等），结果表明 Vision Transformer（ViT）模型可以获得与当前最优卷积网络相媲美的结果，而其训练所需的计算资源大大减少。
# 1 Introduction #
- 受到 NLP 领域中 Transformer 缩放成功的启发，这项研究尝试将标准 Transformer 直接应用于图像，并尽可能减少修改。为此，该研究将图像分割成多个图像块（patch），并将这些图像块的线性嵌入序列作为 Transformer 的输入。然后用 NLP 领域中处理 token 的方式处理图像块，并以监督的方式训练图像分类模型。
- 在中等规模的数据集（如 ImageNet）上训练时，这样的模型产生的结果并不理想，准确率比同等大小的 ResNet 低几个百分点。这个看似令人沮丧的结果是可以预料的：Transformer 缺少一些 CNN 固有的归纳偏置，例如平移同变性和局部性，因此在数据量不足的情况下进行训练后，Transformer 不能很好地泛化
- 但是，如果在大型数据集（14M-300M 张图像）上训练模型，则情况大为不同。该研究发现大规模训练胜过归纳偏置。在足够大的数据规模上进行预训练并迁移到数据点较少的任务时，Transformer 可以获得出色的结果
- 该研究提出的 Vision Transformer 在 JFT-300M 数据集上进行预训练，在多个图像识别基准上接近或超过了 SOTA 水平，在 ImageNet 上达到了 88.36% 的准确率，在 ImageNet ReaL 上达到了 90.77% 的准确率，在 CIFAR-100 上达到了 94.55% 的准确率，在 VTAB 基准 19 个任务中达到了 77.16% 的准确率。
