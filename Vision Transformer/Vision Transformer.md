# Abstract #
- Transformer 架构早已在自然语言处理任务中得到广泛应用，但在计算机视觉领域中仍然受到限制。在计算机视觉领域，注意力要么与卷积网络结合使用，要么用来代替卷积网络的某些组件，同时保持其整体架构不变。
- 该研究表明，对 CNN 的依赖不是必需的，当直接应用于图像块序列时，transformer 也能很好地执行图像分类任务。该研究基于大量数据进行模型预训练，并迁移至多个图像识别基准数据集（ImageNet、CIFAR-100、VTAB 等），结果表明 Vision Transformer（ViT）模型可以获得与当前最优卷积网络相媲美的结果，而其训练所需的计算资源大大减少。
# 1 Introduction #
