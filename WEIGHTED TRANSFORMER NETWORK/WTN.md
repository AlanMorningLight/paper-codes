# 摘要 #
- 具有某种形式的卷积或递归的序列对序列模型。Vaswani等人。（2017）提出一种新的架构，完全避免重复和冲突。相反，它只使用自我关注和前馈层。虽然所提出的体系结构在多个机器翻译任务上取得了最新的结果，但它需要大量的参数和训练过程才能收敛。我们提出了加权transformer，一种改进了注意层的transformer，它不仅在BLEU得分上优于基线网络，而且收敛速度快了15-40%。具体地说，我们用多个自我注意分支代替多个头部注意，这些分支在训练过程中由模型学习组合而成。
# 引言 #
- 在Vaswani等人（2017）中，作者介绍了transformer网络，这是一种新的结构，它避免了递推方程，并仅使用注意将输入序列映射到隐藏状态。具体来说，作者使用的位置编码结合一个多头注意机制。这允许增加并行计算并缩短收敛时间。作者报告了神经机器翻译的结果，结果显示transformer网络在WMT 2014英语-德语和英语-法语任务中实现了最先进的性能，同时比以前的方法快了几个数量级。
- transformer网络仍需要大量参数才能达到最先进的性能。在newstest2013英语到德语的翻译任务中，基本模型需要65M参数，而大型模型需要213M参数。我们提出了一种称为加权transformer的transformer网络，它使用自我注意分支来代替多头部注意。在原transformer网络的注意机制分支代替了多个头部，模型在训练过程中学习将这些分支组合起来。这种分支结构使得网络能够以显著更低的计算成本实现可比较的性能。事实上，通过这一修改，我们在WMT 2014英语对德语和英语对法语任务中的BLEU分数分别提高了0.5分和0.4分。最后，我们提出的证据表明，一个规则化的影响，提出的架构。
# 相关工作 #
- 我们提出了一种改进的Transformer网络，其中多头注意力层被分支的自我注意力层代替。 不同分支的贡献作为训练过程的一部分被训练。
# TRANSFORMER NETWORK #
# PROPOSED NETWORK ARCHITECTURE #
