# 摘要 #
- 具有某种形式的卷积或递归的序列对序列模型。Vaswani等人。（2017）提出一种新的架构，完全避免重复和冲突。相反，它只使用自我关注和前馈层。虽然所提出的体系结构在多个机器翻译任务上取得了最新的结果，但它需要大量的参数和训练过程才能收敛。我们提出了加权transformer，一种改进了注意层的transformer，它不仅在BLEU得分上优于基线网络，而且收敛速度快了15-40%。具体地说，我们用多个自我注意分支代替多个头部注意，这些分支在训练过程中由模型学习组合而成。
# 引言 #
- 在Vaswani等人（2017）中，作者介绍了transformer网络，这是一种新的结构，它避免了递推方程，并仅使用注意将输入序列映射到隐藏状态。具体来说，作者使用的位置编码结合一个多头注意机制。这允许增加并行计算并缩短收敛时间。作者报告了神经机器翻译的结果，结果显示transformer网络在WMT 2014英语-德语和英语-法语任务中实现了最先进的性能，同时比以前的方法快了几个数量级。
- transformer网络仍需要大量参数才能达到最先进的性能。在newstest2013英语到德语的翻译任务中，基本模型需要65M参数，而大型模型需要213M参数。我们提出了一种称为加权transformer的transformer网络，它使用自我注意分支来代替多头部注意。在原transformer网络的注意机制分支代替了多个头部，模型在训练过程中学习将这些分支组合起来。这种分支结构使得网络能够以显著更低的计算成本实现可比较的性能。事实上，通过这一修改，我们在WMT 2014英语对德语和英语对法语任务中的BLEU分数分别提高了0.5分和0.4分。最后，我们提出的证据表明，一个规则化的影响，提出的架构。
# 相关工作 #
- 我们提出了一种改进的Transformer网络，其中多头注意力层被分支的自我注意力层代替。 不同分支的贡献作为训练过程的一部分被训练。
# TRANSFORMER NETWORK #
- 因为模型不包括Recurrence/Convolution，因此是无法捕捉到序列顺序信息的，例如将K、V按行进行打乱，那么Attention之后的结果是一样的。但是序列信息非常重要，代表着全局的结构，因此必须将序列的分词相对或者绝对Position信息利用起来。Position Embedding本身是一个绝对位置的信息。
- （1）缩放点积注意力
- （2）、（3）Multi-Head Attention就是把Scaled Dot-Product Attention的过程做H次，然后把输出Z合起来
- （4）位置前馈网络，在进行了Attention操作之后，Encoder和Decoder中的每一层都包含了一个全连接前向网络，对每个Position的向量分别进行相同的操作，包括两个线性变换和一个ReLU激活输出：
- 关于Transformer网络的一个自然问题是，为什么自我关注应该优先于循环或卷积模型。Vaswani et al.（2017）陈述了偏好的三个原因：（a）每层的计算复杂性，（b）并行性，以及（c）远程依赖之间的路径长度。假设序列长度为n，向量维数为d，对于自注意层，每层的复杂度为O（n2d），而对于循环层，每层的复杂度为O（nd2）。在d>n的情况下，自我注意层的复杂度要比循环层的复杂度低。此外，对于自我注意层，顺序计算的数目是O（1），对于循环层，顺序计算的数目是O（n）。这有助于提高并行计算体系结构的利用率。最后，对于自关注层，依赖关系之间的最大路径长度为O（1），而对于递归层，依赖关系之间的最大路径长度为O（n）。这种差异有助于阻碍递归模型学习长期依赖性的能力。
# PROPOSED NETWORK ARCHITECTURE #
- 在等式（3）和（4）中，我们描述了Vaswani等人提出的attention层。包括多头注意力子层和FFN子层。 对于加权Transformer，我们提出了分支注意力，它可以修改Transformer网络中的整个注意力层（包括多头注意力和前馈网络）。 attention层可以描述为（5）（6）（7）.M总分支数。κi, αi 可学习的参数，FFN定义同（4）。
- 在上面的等式中，κ可以解释为学习的串联权重，α可以解释为学习的附加权重。 实际上，在使用α加权求和之前，κ会缩放各个分支的贡献。 我们确保在每个训练步骤中投影均会尊重所有界限。
- 尽管可以将α和κ合并为一个变量并进行训练，但通过将它们分开，我们发现了更好的训练结果。 它也提高了模型的可解释性，使（α，κ）可以看作是各个分支上的概率质量。
- 可以证明，如果对于所有i，κi= 1且αi= 1，我们将恢复多头注意的方程式（3）。 但是，鉴于？ iκi= 1并且在加权变压器中。 我们提出的架构的一种解释是，它取代了？ iαi= 1界限，这些值是不允许的多头注意力由多分支注意力组成。 与其将不同负责人的贡献并置，不如将它们视为多分支网络学会合并的分支
- 这种机制增加了O（M）可训练的权重。 与已经包含213M重量的模型相比，本机制增加了192个权重这是微不足道的增加。 没有这些额外的可训练权重，提出的机制与“Transformer”中的多头注意机制相同。 所提出的注意机制在编码器层和解码器层中都使用，并且在解码器层中被屏蔽，就像在Transformer网络中一样。 类似地，保留了编码器-解码器层中的位置编码，层归一化和残留连接。 为了清楚起见，我们从图1中消除了这些细节。 除了使用（α，κ）学习权重外，还可以通过softmax层使用专家混合归一化（Shazeer et al。，2017）。 但是，我们发现这比我们提出的要糟糕。
- 与Transformer对所有头部均等地加权不同，所提出的机制可以提高对不同头部的重要性。 反过来，这会优先考虑它们的渐变并简化优化过程。 此外，正如计算机视觉中的多分支网络所知（Gastaldi，2017年），这种机制倾向于使分支学习去相关的输入-输出映射。 这减少了共适应并提高了泛化能力。 
