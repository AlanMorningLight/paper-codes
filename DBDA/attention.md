# 摘要 #
- 序列转导模型主要是基于复杂的递归或卷积神经网络，包括一个编码器和一个解码器。本文提出一个新的简单的网络结构Transformer完全基于注意力机制，不需要递归和卷积。
- 在两个机器翻译任务上的实验表明，该模型的结果较好，并行性更高，所需的训练时间大大减少。在WMT 2014英语-德语翻译任务中达到了28.4 BLEU，比现有的最佳结果提高了超过2个BLEU。在8个GPU上训练3.5天，建立一个最先进的模型41.8BLEU。
# 引言 #



# 模型框架 #
- 大多数竞争性神经序列转导模型都有编解码器结构[5,2,35]。这里，编码器将符号表示的输入序列（x1，…，xn）映射到连续表示序列z=（z1，…，zn）。给定z，解码器然后一次生成一个符号的输出序列（y1，…，ym）。在每一步中，模型都是自回归的[10]，在生成下一步时，使用先前生成的符号作为附加输入
- Transformer遵循这个整体架构，对编码器和解码器使用堆叠的注意和点式的完全连接层，分别显示在图1的左半部和右半部。
## 3.2 ##
- 注意函数可以描述为将一个查询和一组键值对映射到一个输出，其中查询、键、值和输出都是向量。输出被计算为值的加权和，其中分配给每个值的权重由查询与相应键的兼容性函数计算。
### 3.2.1 Scaled Dot-Product Attention ###
- Figure 2:输入包括dk维的查询和键，以及dv维的值。我们使用所有的键计算查询的点积，将每个键除以√dk，并应用softmax函数来获得值的权重。在实践中，我们同时计算一组查询的注意函数，并将其压缩到矩阵Q中。键和值也被打包到矩阵K和V中。我们将输出矩阵计算为：(1)
- 
