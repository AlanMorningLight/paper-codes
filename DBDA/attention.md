# 模型框架 #
- 大多数竞争性神经序列转导模型都有编解码器结构[5,2,35]。这里，编码器将符号表示的输入序列（x1，…，xn）映射到连续表示序列z=（z1，…，zn）。给定z，解码器然后一次生成一个符号的输出序列（y1，…，ym）。在每一步中，模型都是自回归的[10]，在生成下一步时，使用先前生成的符号作为附加输入
- Transformer遵循这个整体架构，对编码器和解码器使用堆叠的注意和点式的完全连接层，分别显示在图1的左半部和右半部。
## 3.2 ##
- 注意函数可以描述为将一个查询和一组键值对映射到一个输出，其中查询、键、值和输出都是向量。输出被计算为值的加权和，其中分配给每个值的权重由查询与相应键的兼容性函数计算。
### 3.2.1 Scaled Dot-Product Attention ###
- Figure 2:输入包括dk维的查询和键，以及dv维的值。我们使用所有的键计算查询的点积，将每个键除以√dk，并应用softmax函数来获得值的权重。在实践中，我们同时计算一组查询的注意函数，并将其压缩到矩阵Q中。键和值也被打包到矩阵K和V中。我们将输出矩阵计算为：(1)
- 
