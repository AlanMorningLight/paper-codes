# 基于双通道双注意机制网络的高光谱图像分类 #
## 摘要 ##
- 为了提高分类精度和减少训练样本，本文提出了一种双分支双注意机制网络（DBDA）。
- DBDA中设计了两个分支来捕捉HSI中包含的大量光谱和空间特征。另外，在这两个分支上分别应用信道注意块和空间注意块，使得DBDA能够对提取的特征映射进行优化和优化。
- 四个高光谱数据集上的一系列实验表明，该框架在训练样本明显不足的情况下，具有优于现有算法的性能。
## 1、引言 ##
## 2、相关论文 ## 
### 2.1 基于3D-Cube的高光谱图像分类 ###
- 与传统的仅使用光谱特征的基于像素的方法不同，基于3D立方体的方法，如SSRN[31]、FDSSC[32]、DBMA[34]和我们提出的框架同时利用光谱和空间信息。基于像素的方法使用单个像素来训练网络，而基于三维立方体的方法则以目标像素及其相邻像素作为输入。当然，相邻中心像素的标签并没有被输入到网络中，我们只探索目标像素周围丰富的空间信息。一般情况下，基于像素的方法与基于三维立方体的方法的不同之处在于前者的输入尺寸为1×1×b，后者的输入尺寸为p×p×b，其中p×p表示相邻像素的数目，b表示光谱带的数目。
### 2.2 带有BN层的3D-CNN ###
- 具有批处理规范化（BN）[45]的3D-CNN是基于3D立方体的深度学习模型中的常见元素。通过输入大量的标记图像，多层非线性层的深度学习模型可以学习分层表示，多层卷积层使CNN能够更有效地学习稀疏约束下的特征。1D-CNN和2D-CNN只使用光谱特征或捕捉像素的局部空间特征。在对包含大量空间和光谱信息的HSI进行分类时，应采用3D-CNN方法进行分类。因此，我们使用3D-CNN作为DBDA的基本结构。此外，我们在每一个3D-CNN层中加入了BN层以提高数值稳定性
### 2.3 ResNet and DenseNet ###
- 通常，卷积层越多，网络性能越好。然而，层数过多可能会导致梯度消失和梯度爆炸的问题变得更糟。ResNet[29]和DenseNet[30]是解决该问题的有效方法。
- Figure 2a 在ResNet中的传统CNN模型中增加了一个跳转连接。如图2a所示，H表示隐藏块，它是一个包含卷积层、激活层和BN层的模块。跳转连接可以看作是身份映射，使输入数据能够直接通过网络传递。剩余块是ResNet中的基本单元，第l个剩余块的输出可以计算为：
- Figure 2b
- Figure 3
### 2.4 注意机制 ###
- 3D-CNN的一个缺点是所有的空间像素和谱带在空间域和谱域中都具有相等的权重。显然，不同的光谱波段和空间像素对特征提取的贡献是不同的。注意机制是解决这一问题的有力手段。受人类视觉感知过程的启发[46]，注意力机制被设计成更多地关注信息区域，而较少考虑非必要区域。注意力机制已被用于图像分类[47]，后来被证明在其他领域表现突出，包括图像标题[48]、文本到图像合成[49]和场景分割[44]等。在DANet[44]中，可以采用信道注意块和空间注意块来增加强制信道和像素的权重。下面将详细介绍这两个模块。
#### 2.4.1 光谱注意块 ####
- 如图4a所示，直接从初始输入A∈Rc×p×p计算信道注意映射X∈Rc×c，其中p×p为输入的块大小，c表示输入信道的个数。具体来说，在A和AT之间进行矩阵乘法运算，为了得到信道注意映射X∈Rc×c，将一个softmax层连接为：
- (6)
- (7)

#### 2.4.2 空间注意块 ####
## 3、方法 ##
- Figure 5 DBDA框架的过程包括三个步骤：数据集生成、训练和验证以及预测
- (10)
### 3.1DBDA网络框架 ###
- Figure 6 为了方便起见，我们将顶部分支称为光谱分支，将底部分支命名为空间分支。将输入分别输入到光谱分支和空间分支，得到光谱特征映射和空间特征映射。然后采用光谱和空间特征映射的融合运算得到分类结果。
- 以印度松（IP）数据集为例，介绍了光谱分支、空间分支和光谱与空间融合操作，并将其patch size 设为9×9×200。为了便于理解下面提到的矩阵，如（9×9×97，24），9×9×97表示三维立方体的高度、宽度和深度，24表示3D-CNN生成的三维立方体的数量。
#### 3.1.1 带有通道注意块的光谱分支 ####
- Table 1 首先，使用1×1×7内核大小的3D-CNN层。下采样步长设置为（1，1，2），能会减少频带数。然后，获取（9×9×97，24）形状的特征地图。在此基础上，附加了3D-CNN与BN相结合的密集谱块。每个稠密谱块的3D-CNN有12个通道，核尺寸为1×1×7。在附加了密集谱块后，特征图的通道增加到60个，由方程（5）计算。因此，我们得到了尺寸为（9×9×97，60）的特征地图。然后，在最后一个核尺寸为1×1×97的3D-CNN后，生成（9×9×1，60）特征映射。然而，60个频道对分类的贡献不同。为了细化频谱特征，采用图4a所示和第2.4.1节中解释的信道注意块。通道注意块强化了信息通道，削弱了缺乏信息的通道。在通过信道注意获得加权谱特征图后，采用了BN层和dropout层来增强数值稳定性和克服过拟合。最后，通过一个全局平均池层，得到1×60形状的特征地图。光谱分支的实现如表1所示。
#### 3.1.2 带有空间注意块的空间分支 ####
- Table 2  同时，将输入的数据以9×9×200的形式传送到空间分支，初始3D-CNN层的大小设置为1×1×200，可以将光谱带压缩成一维。得到了（9×9×1，24）形状的特征图。然后，附加了三维CNN与BN相结合的密集空间块。密集谱块中的每个3D-CNN有12个通道，核尺寸为3×3×1。接下来，将提取的（9×9×1，60）形状的特征图输入到空间注意块中，如图4b所示，并在第2.4.2节中阐述。在注意块的基础上，对每个像素的系数进行加权，以获得更具区分性的空间特征。在获取加权的空间特征图后，应用带有dropout层的BN层。最后通过全局平均池化层得到1×60的空间特征图。空间分支的实现如表2所示。
#### 3.1.3 HSI分类的空谱结合 ####
- Figure 7 通过谱分支和空间分支，得到多个光谱特征图和空间特征图。然后，我们在两个特征之间执行连接以进行分类。此外，采用级联运算代替加法运算的原因是光谱和空间特征都在不相关的域中，并且级联运算可以保持它们的独立性，而加法运算则会将它们混合在一起。最后通过全连通层和softmax激活函数得到分类结果
### 3.2避免过拟合 ###
- 参数过多，样本数量有限导致过拟合
#### 3.2.1激活函数 ####
- （11）激活函数将非线性的概念引入神经网络。适当的激活函数可以加快网络的反向传播和收敛速度。我们采用的激活函数是Mish[50]，一个自正则化的非单调激活函数，而不是传统的ReLU（x）=max（0，x）[51]。Mish的公式为：
- (12)
- Figure 8.ReLU是一个分段线性函数，它对所有的负输入进行修剪。因此，如果输入是非正的，那么神经元将“死亡”并且不能再被激活，即使负输入可能包含有用的信息。相反，Mish将负输入保留为负输出，较好地交换了输入信息和网络稀疏性
#### 3.2.2Dropout层，提前停止策略和动态学习率调整 ####
- （13）
## 4、实验结果 ##
- 为了验证该模型的准确性和有效性，设计了四个数据集的实验来比较和验证所提出的网络与其他方法的准确性和效率。用总体准确度（OA）、平均准确度（AA）和Kappa系数（K）三个量化指标来衡量每种方法的准确度。具体地说，OA表示整个像素的真实分类的比率。AA表示所有类别的平均准确度。Kappa系数反映了地面真实性与分类结果的一致性。三个度量值越高，分类效果越好。同时，我们研究了每个框架的运行时间，以评估其效率
- 对于每个数据集，从标注的数据中随机抽取一定数量的训练样本和验证样本，其余样本用于测试模型的性能。由于该方法在训练样本严重不足的情况下仍能保持良好的性能，所以训练样本和验证样本的数量被设定在最小的水平上。
### 4.1 数据集 ###
- Table 3 印度松（IP）数据集的每类训练、验证和测试样本
- Table 4 Pavia大学（UP）数据集的每类培训、验证和测试样本。
- Table 5 Salinas Valley（SV）数据集的每类培训、验证和测试样本
- Table 6 博茨瓦纳数据集（BS）数据集的每类培训、验证和测试样本。
### 4.2 实验设置 ###
- 为了评估DBDA的有效性，我们将基于深度学习的分类器CDCNN[27]、SSRN[31]、FDSSC[32]和最先进的双分支多注意机制网络（DBMA）[34]与我们提出的框架进行了比较。此外，还考虑了RBF核的支持向量机[9]。每个分类器的batch size大小根据其原始论文来设置。为了比较训练和测试的时间消耗，所有实验都在配置了32gb内存和nvidiageforcertx2080tigpu的同一平台上进行。所有基于深度学习的分类器用PyTorch实现，支持向量机用sklearn实现。然后，分别对上述方法进行简要介绍
- 支持向量机：对于具有径向基函数（RBF）核的支持向量机，所有单个像素及其谱带都直接输入。CDCNN：CDCNN的体系结构如[27]所示，它基于2D-CNN和

ResNet公司。输入尺寸为5×5×b，其中b表示光谱带数。SSRN:SSRN的体系结构是在[31]中提出的，它是基于3D-CNN和ResNet的。输入的大小为7×7×b。

FDSSC:FDSSC的体系结构见[32]，它基于3D-CNN和DenseNet。输入的大小为9×9×b。

DBMA:DBMA的体系结构如[34]所示，它基于3D-CNN、DenseNet和一种注意力机制。7×7×b是输入的补丁大小。

对于CDCNN、SSRN、FDSSC、DBMA和所提出的方法，批大小设置为16，优化器设置为Adam，学习率为0.0005。早期停止策略的上限设置为200个时期。如果验证集中的损失在20个时期内不再下降，那么我们将终止训练阶段。
### 4.3 分类图和分类结果 ###
#### 4.3.1 IP 数据结果 ####
- Table 7 具有3%训练样本的IP数据集的分类结果
- Figure 9 使用3%训练样本的IP数据集的分类图。（a） 假彩色图像。（b） GT。（c–h）不同算法的分类图。
#### 4.3.2 UP 数据结果 ####
- Table 8 具有3%训练样本的UP数据集的分类结果
- Figure 10 使用3%训练样本的UP数据集的分类图。（a） 假彩色图像。（b）GT。（c–h）不同算法的分类图。
#### 4.3.3 SV 数据结果 ####
- Table 9 具有3%训练样本的SV数据集的分类结果
- Figure 11 使用3%训练样本的SV数据集的分类图。（a） 假彩色图像。（b） GT。（c–h）不同算法的分类图。
#### 4.3.4 BS 数据结果 ####
- Table 10 具有3%训练样本的BS数据集的分类结果
- Figure 12 使用3%训练样本的BS数据集的分类图。（a） 假彩色图像。（b） GT。（c–h）不同算法的分类图。
### 4.4 运行时间 ###
- 以上实验证明，我们提出的方法可以用较少的数据获得更高的精度。然而，一个好的方法应该在准确度和效率之间取得适当的平衡。这一部分的执行是为了衡量每种方法的效率。表11-14列出了IP、UP、SV和BS数据集上六种算法的时间消耗。
- 由于我们使用支持向量机作为基于像素的模型，所以在大多数情况下它比基于三维立方体的模型花费的时间更少。由于2D-CNN包含的待训练参数较少，CDCNN比基于3D-CNN的模型耗时更少
- Table 11
- Table 12
- Table 13
### 5、讨论 ### 
#### 5.1 训练样本比例 ####
- 
