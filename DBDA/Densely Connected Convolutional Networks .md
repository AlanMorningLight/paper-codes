# 摘要 #
- 如果卷积网络在靠近输入层和接近输出层的层之间包含较短的连接，那么卷积网络可以更深入、更准确、更有效地进行训练。本论文引入了密集卷积网络（DenseNet），它以前馈方式将每一层连接到另一层。传统的L层卷积网络在每层及其后续层之间有L个连接，我们的网络有L（L+1）/2个直接连接。对于每一层，所有前一层的特征映射被用作输入，其自身的特征映射被用作所有后续层的输入。
- DenseNet有几个优点：它们缓解了消失梯度问题，加强了特征传播，鼓励了特征的重用，并大大减少了参数的数量
# 引言 #
# 相关论文 #
- Densenet通过特征重用来挖掘网络的潜力，产生易于训练和高效参数的密集模型。将不同层学习到的特征映射连接起来，可以增加后续层的输入变化，并提高效率。
# DenseNets #
- 考虑通过卷积网络传递的单个图像x0。该网络由L层组成，每层实现一个非线性变换Hl（·），l索引图层。Hl（·）可以是操作的复合函数，例如批处理标准化（BN）[14]、校正线性单元（ReLU）[6]、池[19]或卷积（Conv）.第l层的输出是xl。
- ResNets 传统卷积前馈网络的输出连接起来？作为（？）输入的第th层？+1）第[16]层，产生以下层过渡：x？=H？（x？−1）。ResNets[11]添加跳过连接，该连接绕过具有标识函数的非线性转换：
- Figure 1 展示了生成的DenseNet的布局。 为了进一步改善层与层之间的信息流，我们提出了一种不同的连接模式：我们引入从任何层到所有后续层的直接连接。
- 复合函数。受[12]的启发，我们定义H？（·）作为三个连续操作的复合函数：批处理标准化（BN）[14]，然后是一个校正的线性单元（ReLU）[6]和3×3卷积（Conv）。
- 池层。（2）当特征的大小不可行时，使用Eq操作。然而，卷积网络的一个重要组成部分是向下采样层来改变特征映射的大小。为了便于在我们的架构中进行下采样，我们将网络划分为多个紧密连接的密集块；见图2。我们将块之间的层称为转换层，用于进行卷积和池化。在我们的实验中使用的过渡层由一个批正化层和一个1×1的卷积层和一个2×2的平均池层组成。
- 增长率。如果每个函数H？生成k个特征图，那么？第层有k0+k×（？−1）输入特征映射，其中k0是输入层中的通道数。DenseNet与现有网络体系结构的一个重要区别是，DenseNet可以有非常窄的层，例如k=12。我们称超参数k为网络的增长率。我们在第4节中说明，相对较小的增长率足以在我们测试的数据集上获得最新的结果。对此的一种解释是，每一层都可以访问其块中的所有前面的特征地图，因此，可以访问网络的“集体知识”。我们可以将特征图视为网络的全局状态。每个图层都将自己的k个要素地图添加到此状态。增长率决定了每一层向全球国家贡献了多少新信息。全局状态一旦写入，就可以从网络中的任何地方访问，而且与传统的网络体系结构不同，不需要在层到层之间复制它。
- 
- 瓶颈层。虽然每个层只产生k个输出特征映射，但它通常有更多的输入。文献[37，11]指出，可以在每个3×3卷积之前引入1×1卷积作为瓶颈层，以减少输入特征映射的数量，从而提高计算效率。我们发现这种设计对DenseNet特别有效，我们参考了我们的网络，即BN ReLU Conv（1×1）-BN ReLU Conv（3×3）版本的h？在我们的实验中，我们让每个1×1的卷积产生4k特征映射。
- 压缩。为了进一步提高模型的紧凑性，我们可以减少过渡层特征映射的数量。如果一个稠密的块包含m个特征映射，我们让下面的过渡层生成？θm？输出特征映射，其中0<θ≤1被称为压缩因子。当θ=1时，跨过渡层的特征图数量保持不变。我们把θ<1的DenseNet称为DenseNet-C，我们在实验中设置θ=0.5。当同时使用θ<1的瓶颈层和过渡层时，我们将我们的模型称为DenseNet-BC
