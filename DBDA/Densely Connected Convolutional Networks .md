# 摘要 #
- 如果卷积网络在靠近输入层和接近输出层的层之间包含较短的连接，那么卷积网络可以更深入、更准确、更有效地进行训练。本论文引入了密集卷积网络（DenseNet），它以前馈方式将每一层连接到另一层。传统的L层卷积网络在每层及其后续层之间有L个连接，我们的网络有L（L+1）/2个直接连接。对于每一层，所有前一层的特征映射被用作输入，其自身的特征映射被用作所有后续层的输入。
- DenseNet有几个优点：它们缓解了梯度消失问题，加强了特征传播，鼓励了特征的重用，并大大减少了参数的数量
# 引言 #
- Figure 1:在本文中为了确保网络中各层之间的最大信息流，我们将所有层（具有匹配的特征映射大小）直接相互连接。为了保持前馈特性，每个层从所有前一层获取额外的输入，并将其自身的特征映射传递给所有后续层。图1示意性地说明了这个布局。关键的是，与resnet不同的是，在特征被传递到一个层之前通过求和来组合特性；相反，我们通过连接它们来组合特性。所以第l层有l个输入，由前面所有卷积块的特征映射组成。它自己的特征地图都传给了所有后续的L-l层。这在L层网络中引入了L（L+1）2连接。由于其稠密的连接模式，我们将我们的方法称为稠密卷积网络（DenseNet）。
- ResNets的参数数目要大得多，因为每个层都有自己的权重。我们提出的DenseNet架构明确区分了添加到网络中的信息和保留的信息。DenseNet层非常窄（例如，每层12个过滤器），只在网络的“集合知识”中添加一小组特征映射，并保持其余特征映射不变，最终分类器根据网络中的所有特征映射做出决策.所以DenseNet参数数目小。
- 除了更好的参数效率外，DenseNets的一大优势是它改进了整个网络的信息流和梯度，这使得他们易于训练。每层都可以直接访问损失函数和原始输入信号的梯度，从而实现深度监控[20]。这有助于对更深层次的网络体系结构进行训练。此外，我们还观察到dense连接具有正则化效应，这可以缓解小训练集的过拟合
# 相关论文 #
- 不止于网络的深度和广度，Densenet通过特征重用来挖掘网络的潜力，产生易于训练和高效参数的密集模型。将不同层学习到的特征映射连接起来，可以增加后续层的输入变化，并提高效率。这是DenseNets 和 ResNets最大的区别。
# DenseNets #
- 考虑通过卷积网络传递的单个图像x0。该网络由L层组成，每层实现一个非线性变换Hl（·），l索引图层。Hl（·）可以是操作的复合函数，例如批处理标准化（BN）[14]、校正线性单元（ReLU）[6]、池层[19]或卷积（Conv）.第l层的输出是xl。
- ResNets 传统卷积前馈网络将第l层的输出连接起来作为第（l+1）层输入，产生以下层过渡：xl=Hl（xl−1）。ResNet网络的优点是梯度可以通过映射函数从后一层流向前一层。然而，映射函数和Hl的输出通过求和的方式组合在一起，这可能会阻碍网络中的信息流。
- Dense connectivity Figure 1 展示了生成的DenseNet的布局。 为了进一步改善层与层之间的信息流，我们提出了一种不同的连接模式：我们引入从任何层到所有后续层的直接连接。
- 复合函数。受[12]的启发，我们定义Hl（·）作为三个连续操作的复合函数：批处理标准化（BN）[14]，然后是一个校正的线性单元（ReLU）[6]和3×3卷积（Conv）。
- 池层。使用Eq（2）操作当特征的大小不可行时，。然而，卷积网络的一个重要组成部分是向下采样层来改变特征映射的大小。为了便于在我们的架构中进行下采样，我们将网络划分为多个紧密连接的密集块；见图2。我们将块之间的层称为转换层，用于进行卷积和池化。在我们的实验中使用的过渡层由一个批正化层和一个1×1的卷积层和一个2×2的平均池层组成。
- 增长率。如果每个函数Hl生成k个特征图，第l层有k0+k×（l−1）输入特征映射，其中k0是输入层中的通道数。DenseNet与现有网络体系结构的一个重要区别是，DenseNet可以有非常窄的层，例如k=12。我们称超参数k为网络的增长率。我们在第4节中说明，相对较小的增长率足以在我们测试的数据集上获得最新的结果。对此的一种解释是，每一层都可以访问其块中的所有前面的特征图，因此，可以访问网络的“集体知识”。我们可以将特征图视为网络的全局状态。每个图层都将自己的k个特征图添加到此状态。增长率决定了每一层向全局状态贡献了多少新信息。全局状态一旦写入，就可以从网络中的任何地方访问，而且与传统的网络体系结构不同，不需要在层到层之间复制它。
- 瓶颈层。虽然每个层只产生k个输出特征映射，但它通常有更多的输入。文献[37，11]指出，可以在每个3×3卷积之前引入1×1卷积作为瓶颈层，以减少输入特征映射的数量，从而提高计算效率。我们发现这种设计对DenseNet特别有效，我们参考了我们的网络，即BN ReLU Conv（1×1）-BN ReLU Conv（3×3）版本的h？在我们的实验中，我们让每个1×1的卷积产生4k特征映射。
- 压缩。为了进一步提高模型的紧凑性，我们可以减少过渡层特征映射的数量。如果一个稠密的块包含m个特征映射，我们让下面的过渡层生成？θm？输出特征映射，其中0<θ≤1被称为压缩因子。当θ=1时，跨过渡层的特征图数量保持不变。我们把θ<1的DenseNet称为DenseNet-C，我们在实验中设置θ=0.5。当同时使用θ<1的瓶颈层和过渡层时，我们将我们的模型称为DenseNet-BC
- 实施细节。在除Ima-geNet之外的所有数据集中，我们实验中使用的DenseNet有三个密集的块，每个块都有相同数量的层。在进入第一个稠密块之前，对输入图像执行16个（或两倍于DenseNet-BC的增长率）输出通道的卷积。对于核尺寸为3×3的卷积层，输入的每边被零填充一个像素，以保持特征映射的大小不变。我们使用1×1卷积和2×2平均池作为两个相邻密集块之间的过渡层。在最后一个稠密块的末尾，执行一个全局平均池，然后附加一个softmax分类器。三个稠密块的特征图尺寸分别为32×32、16×16和8×8。我们实验了具有{L=40，k=12}，{L=100，k=12}和{L=100，k=24}的基本DenseNet结构。对于DenseNet-BC，计算了{L=100，k=12}，{L=250，k=24}和{L=190，k=40}的网络
- Table 1:ImageNet的DeseNet架构。所有网络的增长率为k=32。注意，表中显示的每个“conv”层对应于序列BN-ReLU-conv。
